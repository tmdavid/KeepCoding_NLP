{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BoW and n-grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bag of Words representations. Word n-grams. Char-ngrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\n",
    "    'frase en castellano',\n",
    "    'english sentence',\n",
    "    'esta frase no esta en english',\n",
    "    'estos documentos tienen muy poco sentido',\n",
    "    'el documento es un conjunto de frases',\n",
    "    'Vim es mucho mejor que emacs.',\n",
    "    'tabs vs spaces'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bag of Words (BoW) ->  1-grams.\n",
    "\n",
    "histogram of the words within the text -> Count the appearences of each word \n",
    "\n",
    "How do we get the vocabulary? Simply map words from training set to a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_vocabulary_maps(docs):\n",
    "    vocabulary = {}\n",
    "    inverse_vocabulary = {}\n",
    "    for doc in documents:\n",
    "        for token in doc.split(' '):\n",
    "            if token not in vocabulary:\n",
    "                vocabulary[token] = len(vocabulary)\n",
    "                inverse_vocabulary[len(inverse_vocabulary)] = token\n",
    "    return vocabulary, inverse_vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'muy': 10, 'tienen': 9, 'el': 13, 'mucho': 21, 'frases': 19, 'que': 23, 'en': 1, 'emacs.': 24, 'english': 3, 'vs': 26, 'esta': 5, 'frase': 0, 'no': 6, 'documentos': 8, 'de': 18, 'conjunto': 17, 'sentence': 4, 'castellano': 2, 'estos': 7, 'mejor': 22, 'documento': 14, 'spaces': 27, 'Vim': 20, 'es': 15, 'tabs': 25, 'un': 16, 'sentido': 12, 'poco': 11}\n"
     ]
    }
   ],
   "source": [
    "vocabulary, inverse_vocabulary = generate_vocabulary_maps(documents)\n",
    "print(vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "numpy makes representing this type of data incredibly easy. like C arrays.\n",
    "\n",
    "When working with data that we may use as features, try as soon as possible to work with numpy arrays.\n",
    "\n",
    "Matrix Manipulation is fairly important along the course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0], dtype=int32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_representation_np = np.zeros((len(vocabulary)), dtype='int32')\n",
    "new_representation_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "string = 'documento en castellano'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(x, vocab):\n",
    "    assert type(x) == str, 'wrong type. x must be a sentence'\n",
    "    new_representation_np = np.zeros((len(vocab)), dtype='int32')\n",
    "    idx = [vocab[token] for token in x.split(' ')]\n",
    "    new_representation_np[idx] = 1\n",
    "    return new_representation_np    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0], dtype=int32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transform(string, vocab=vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_docs = [\n",
    "    'este documento es de programacion en castellano',\n",
    "    'castellano documento en en',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def itransform(X, vocab):\n",
    "    assert type(X)==list, 'X must be a list'\n",
    "    cols = len(X)\n",
    "    rep = np.zeros((cols, len(vocab)), dtype='int32')\n",
    "    for i, x in enumerate(X):\n",
    "        tokens = [vocab[token] for token in x.split(' ') if token in vocab]\n",
    "        for t in tokens:\n",
    "            rep[i, t] += 1\n",
    "    return rep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0],\n",
       "       [0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0]], dtype=int32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "itransform(new_docs, vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = documents + new_docs\n",
    "count_docs = itransform(docs, vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BoW makes us lose context. No sequence. BoW method is the most basic method we have, and for some tasks its a \"good enough\" baseline.\n",
    "\n",
    "# n-grams\n",
    "\n",
    "parejas, trios, etc etc\n",
    "\n",
    "No es lo mismo una palabra al inicio de una frase, que quizas al en medio!\n",
    "\n",
    "e.g:\n",
    "\n",
    "> Que puedo hacer?\n",
    "\n",
    "> Me dijo que queria hacer algo.\n",
    "\n",
    "En un problema de clasificación quizás es importante saber ordenes, aunque sean parciales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "<SOS>\n",
    "<EOS>\n",
    "\"\"\"\n",
    "from collections import Counter\n",
    "\n",
    "def compute_ngrams(docs, min_n=1, max_n=2, ngrams={}):\n",
    "    assert max_n>min_n, 'max ngram must be bigger than min ngram'\n",
    "    ngrams = ngrams if ngrams else {i:Counter() for i in range(min_n, max_n+1)} \n",
    "    for doc in docs:\n",
    "        doc = '<SOS> ' + doc + ' <EOS>'\n",
    "        tokenized_doc = doc.split(' ')\n",
    "        for ix in range(len(tokenized_doc)):\n",
    "            ngrams_doc = [\" \".join(tokenized_doc[ix:ix+i]) for i in range(min_n, max_n+1) if ix+i < len(tokenized_doc)+1]\n",
    "            for i, ngram in enumerate(ngrams_doc):\n",
    "                ngrams[i+1][ngram]+=1\n",
    "    return ngrams   \n",
    "\n",
    "def update_ngrams(docs, ngrams, min_n=False, max_n=False):\n",
    "    min_n = min_n if min_n else min(ngrams.keys())\n",
    "    max_n = max_n if max_n else max(ngrams.keys())\n",
    "    return compute_ngrams(docs, min_n, max_n, ngrams)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('<EOS>', 9), ('<SOS>', 9)]\n",
      "[('documento es', 2), ('castellano <EOS>', 2)]\n",
      "[('<EOS>', 18), ('<SOS>', 18)]\n",
      "[('documento es', 4), ('castellano <EOS>', 4)]\n"
     ]
    }
   ],
   "source": [
    "ngrams = compute_ngrams(docs)\n",
    "for k in ngrams.keys():\n",
    "    print(ngrams[k].most_common(2))\n",
    "ngrams = update_ngrams(docs, ngrams)\n",
    "for k in ngrams.keys():\n",
    "    print(ngrams[k].most_common(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este punto ya tenemos unas features basicas para poder realizar tareas de clasificacion de textos con un clasificador como bayes por ejemplo.\n",
    "\n",
    "Solo nos quedaria mapear esto a un numpy array. En estos ejemplos todo es magnifico. Realidad? Vectores enormes! Explosión de features! Que pasa con vocabularios enormes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_feature_maps(features):\n",
    "    \"\"\"\n",
    "    Adapatar a las estructuras de cada uno, i.e cada proyecto.\n",
    "    Podriamos estar interesados en el inverso.   \n",
    "    \"\"\"\n",
    "    feature_map = {}\n",
    "    for ngram in features.values():\n",
    "        for token in ngram.keys():\n",
    "            feature_map[token] = len(feature_map)\n",
    "    return feature_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_map = generate_feature_maps(ngrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hay 9 frases\n",
      "Hemos generado 82 features\n",
      "Solo teniamos 30 palabras unicas\n",
      "Comprobacion con ngrams 30 quitando <SOS> y <EOS>\n"
     ]
    }
   ],
   "source": [
    "print('Hay', len(docs), 'frases')\n",
    "print('Hemos generado', len(feature_map), 'features')\n",
    "print('Solo teniamos', len(set([token for doc in docs for token in doc.split(' ')])), 'palabras unicas')\n",
    "print('Comprobacion con ngrams', len(ngrams[1])-2, 'quitando <SOS> y <EOS>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_feature_vector(docs, vocab, min_n=1, max_n=3):\n",
    "    feature_vector = np.zeros((len(docs), len(vocab)), dtype='int32')\n",
    "    for i, doc in enumerate(docs):\n",
    "        doc = '<SOS> ' + doc + ' <EOS>' # tendria que estar hecho en el preproceso de la frase esto!\n",
    "        tokenized_doc = doc.split(' ')\n",
    "        print(tokenized_doc)\n",
    "        for ix in range(len(tokenized_doc)):\n",
    "            ngrams_doc = [\" \".join(tokenized_doc[ix:ix+i]) for i in range(min_n, max_n+1) if ix+i < len(tokenized_doc)+1]\n",
    "            print(ngrams_doc)\n",
    "            maped = [vocab[ngram] for ngram in ngrams_doc if ngram in vocab]\n",
    "            for ngram in maped:\n",
    "                feature_vector[i, ngram] += 1\n",
    "    return feature_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<SOS>', 'nuevo', 'documento', '<EOS>']\n",
      "['<SOS>', '<SOS> nuevo', '<SOS> nuevo documento']\n",
      "['nuevo', 'nuevo documento', 'nuevo documento <EOS>']\n",
      "['documento', 'documento <EOS>']\n",
      "['<EOS>']\n"
     ]
    }
   ],
   "source": [
    "test_docs = ['nuevo documento']\n",
    "test_vector = generate_feature_vector(test_docs, feature_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se generan vectores enormes con con muchos 0 y pocos 1. Más adelante veremos como dejar de tener vectores tan grandes. Hay dos tipos de representación en vectors, sparse, que es la que tenemos aquí y la dense que la veremos mas adelante en PCA/SVD o word embeddings.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 6)\t1\n",
      "  (0, 24)\t1\n",
      "  (0, 31)\t1\n"
     ]
    }
   ],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "sparsified = csr_matrix(test_vector)\n",
    "print(sparsified)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:red\"> NLP tiene un gran problema que acabamos de ver. El vocabulario es ~infinito. se puede generar vocabulario para tu train data, se puede generar vocabulario con las palabras mas usadas en un idioma. Es complicadísimo generar un vocabulario general para una tarea que contenga NLP.</span>\n",
    "\n",
    "## es MUY importante en nuestros pipelines de NLP intentar controlar el vocabulario de alguna manera.\n",
    "\n",
    "Hay varias maneras de hacerlo e iremos viendolas a lo largo de las sesiones. Ya hemos visto una (distancia de edición) para intentar corregir errores ortográficos. En problemas de clasificación con metodos de representación más clásicos, usaremos smoothing. En deep learning hay la tendicia de \"unkificarlo todo\", es decir todos aquellos tokens que desconocemos, tratarlos con un token especial < UNK >\n",
    "\n",
    "Ahora veremos lo mismo de antes con la libreria sklearn, muy recomendable abusar de ella tanto para feature extraction como para clasificación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'analyzer': 'word',\n",
       " 'binary': False,\n",
       " 'decode_error': 'strict',\n",
       " 'dtype': numpy.int64,\n",
       " 'encoding': 'utf-8',\n",
       " 'input': 'content',\n",
       " 'lowercase': True,\n",
       " 'max_df': 1.0,\n",
       " 'max_features': None,\n",
       " 'min_df': 1,\n",
       " 'ngram_range': (1, 3),\n",
       " 'preprocessor': None,\n",
       " 'stop_words': None,\n",
       " 'strip_accents': None,\n",
       " 'token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       " 'tokenizer': None,\n",
       " 'vocabulary': None}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(ngram_range=(1,3))\n",
    "vectorizer.fit(documents)\n",
    "vectorizer.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n",
      "  (0, 6)\t1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<1x73 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 1 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(vectorizer.transform(['nuevo documento']).toarray())\n",
    "print(vectorizer.transform(['nuevo documento']))\n",
    "vectorizer.transform(['nuevo documento'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es muy fácil de usar, bastante eficiente y 100% integrable con otras librerias, ya que devuelve numpy arrays, con son la base para muchas otras librerías.\n",
    "\n",
    "En caso de que el vocabulario sea enorme, en lugar de CountVectorizer usariamos http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.HashingVectorizer.html#sklearn.feature_extraction.text.HashingVectorizer .\n",
    "\n",
    "Más eficiente a nivel de memoria, pero con el drawback de que no podriamos sacar el inverso de las features que calcula. De todas formas, pronto veremos que los one hot vectors, se usan cada vez menos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# smoothing\n",
    "\n",
    "Hay diferentes formas de hacer smoothing. Laplacian smoothing es equivalente a añadir +1 a todos los conteos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tf-idf\n",
    "term frequency: conteo de palabras\n",
    "\n",
    "inverse document frequency: discriminar las mas unicas\n",
    "\n",
    "stop words: palabras tan comunes que no aportan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:MythNLP]",
   "language": "python",
   "name": "conda-env-MythNLP-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
